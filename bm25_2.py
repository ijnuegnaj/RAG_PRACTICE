# -*- coding: utf-8 -*-
"""BM25_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BasUXctJfT3B0rBV6ZpKQdN06M-96119
"""

!pip -q install langchain openai tiktoken langchain-community rank_bm25 sentence-transformers chromadb

import openai, tiktoken
#재귀적 텍스트 분할
from langchain.text_splitter import RecursiveCharacterTextSplitter
#웹주소로 데이터를 load
from langchain.document_loaders import WebBaseLoader
#허깅페이스임베딩, BM25, 앙상블 리트리버
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_community.vectorstores import FAISS
from langchain.vectorstores import Chroma

#토크나이저 만들기
#cl100k_base : GPT와 유사한 형식의 토크나이저를 활용
tokenizer = tiktoken.get_encoding('cl100k_base')
def tiktoken_len(text):
  tokens = tokenizer.encode(text)
  return len(tokens)

model_hug = HuggingFaceEmbeddings(model_name='BAAI/bge-m3')

loaders=[
    WebBaseLoader('https://en.wikipedia.org/wiki/Retrieval-augmented_generation'),
    WebBaseLoader('https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)')
]

docs = []
for load in loaders:
  docs.extend(load.load_and_split())

#청크화
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,
    chunk_overlap=20,
    length_function=tiktoken_len
)
texts = text_splitter.split_documents(docs)

#1.bm25_retriever, 유사도가 가장 높은 3개의 답변을 return함
bm25_r = BM25Retriever.from_documents(texts)
bm25_r.k = 3

#2.chorma vector 리트리버, 유사도가 가장 높은 3개의 답변을 return
chroma_r = Chroma.from_documents(texts, model_hug)
chroma_r = chroma_r.as_retriever(search_kwargs={'k':3})

#앙상블
ensemble = EnsembleRetriever(retrievers=[bm25_r, chroma_r])

docs = ensemble.invoke('difference between fine-tuning and RAG')

docs[0]

import os
from google.colab import userdata
os.environ['OPENAI_API_KEY'] =  userdata.get('ssu')

from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

llm = ChatOpenAI()
question = 'What is the top 3 difference between fine-tuning and RAG?'
qa_chain = RetrievalQA.from_chain_type(llm,
                                       chain_type='stuff',
                                       retriever=ensemble)
result = qa_chain.run(question)
print(result)

!pip install tavily-python

!pip install langchain-openai

#정해진 양식으로 input을 넣을 수 있도록 조절
from langchain_core.prompts import ChatPromptTemplate
#중요!!! pydantic, typing => 데이터 타입을 정의하는것을 도와주는 라이브러리..
#LLM, 딥러닝 '서빙' (서비스화)하는 과정에서 인터넷을 통해 데이터를 주고받음
#데이터의 '데이터 타입'이 원했던 타입이 아니라면 -> 오류 발생 가능성이 높다
#pydantic, typing을 통해서 내가 받기를 원하는 데이터의 데이터 타입을 미리 지정
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain.chat_models import ChatOpenAI

#pydantic의 BaseModel을 상속받아서 포맷팅
class GradeDocuments(BaseModel):
  binary_score : str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
  )

#실제 대화를 구성
llm = ChatOpenAI(model_name = 'gpt-3.5-turbo')
#구조화된 llm > LLM 응답을 지정된 구조로 파싱/검증하는 기능
structured_llm_grader = llm.with_structured_output(GradeDocuments)
system = """You are a grader assessing relevance of a retrieved document to a user question. \n
    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \n
    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question."""
grade_prompt = ChatPromptTemplate.from_messages(
    [
        ('system', system),
        ('human', 'Retrieved Documents : {document}, User Question : {question}')
    ]
)