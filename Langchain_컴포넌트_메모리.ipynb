{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+7u1G8yyK8I7/HOiUoCed",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ijnuegnaj/RAG_PRACTICE/blob/practice2/Langchain_%EC%BB%B4%ED%8F%AC%EB%84%8C%ED%8A%B8_%EB%A9%94%EB%AA%A8%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. LangChain 컴포넌트 - 메모리"
      ],
      "metadata": {
        "id": "U2UXIS7dS6iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4xTwb11VTAoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#노트북 설치\n",
        "#langchain-community : langchain(오픈소스) 커뮤니티의 라이브러리\n",
        "!pip install -q langchain openai langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI9tnTclS77d",
        "outputId": "29828598-9593-4855-aeed-5c743433ceec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('ssu')\n",
        "\n",
        "#openai에 바로 연결\n",
        "#import openai\n",
        "#openai.api_key = '키 값 하드코딩'"
      ],
      "metadata": {
        "id": "HmxEXknsTxD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#openai와 채팅을 수행\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "#체인 > LangChain 컴포넌트의 하나\n",
        "#LangGraph : (생성형) ai로 할 일들을 -> 스텝으로 쪼개서 -> 이걸 그래드형태로 연결 -> 실행시키는 것\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "#메모리\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.memory import ConversationTokenBufferMemory\n",
        "from langchain.memory import ConversationSummaryMemory"
      ],
      "metadata": {
        "id": "UFg9NEYfUdwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#채팅방\n",
        "llm = ChatOpenAI(model_name = 'gpt-3.5-turbo', temperature=0.5)\n",
        "\n",
        "#대화를 체인으로 만듬\n",
        "#'체인'이 된 대화(conversation) + 메모리\n",
        "#메모리 4가지 종류 중에 '버퍼 메모리'\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose = True #chain에서 오가는 대화의 기록을 보여줌\n",
        ")\n",
        "\n",
        "conversation.predict(input='안녕. 내 이름은 은지야')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "XA_nitsJVdwj",
        "outputId": "c6b9effb-2955-4d62-b0d7-3e760ab74777"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: 안녕. 내 이름은 은지야\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요, 은지님! 만나서 반가워요. 저는 OpenAI의 인공지능 언어모델 GPT-3입니다. 어떻게 도와드릴까요?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#메모리 교체\n",
        "#k = 몇 천의 윈도우를 기억할 것인가?(n)\n",
        "memory_window = ConversationBufferWindowMemory(k=1)\n",
        "\n",
        "conv = ConversationChain(\n",
        "    llm = llm, #Chain에서 사용할 GPT와의 단톡 연결\n",
        "    memory = memory_window, #Chain에서 공유할 메모리\n",
        "    verbose = True #Chain에서 오가는 대화의 백그라운드를 보여줌\n",
        ")\n",
        "conv.predict(input='안녕. 내가 좋아하는 색은 청록색이야')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "F-j5v3gUVzPv",
        "outputId": "4b23979e-5fbc-455a-dbef-3ccdf4762063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3766939636.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory_window = ConversationBufferWindowMemory(k=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: 안녕. 내가 좋아하는 색은 청록색이야\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'안녕하세요! 청록색은 정말 아름다운 색이에요. 청록색은 청색과 녹색이 섞인 색으로, 자연에서도 많이 볼 수 있어요. 예를 들어, 푸른 바다와 초록 나무들이 만나는 풍경이 청록색을 연상시킬 수 있어요. 청록색은 신비로운 느낌을 주기도 하고, 상쾌한 느낌을 주기도 해요. 여러분들이 좋아하는 색을 알려주셔서 감사해요. 저도 여러분들이 좋아하는 색을 알고 싶어요. 혹시 여러분들이 좋아하는 색은 무엇인가요?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv.predict(input='숭실대학교에 대해 잘 알아? 숭실대 근처 맛집 추천좀')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "u6YfyO5EbA75",
        "outputId": "afff7ac9-2069-4814-83b5-c1b5fc70bc1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 안녕. 내가 좋아하는 색은 청록색이야\n",
            "AI: 안녕하세요! 청록색은 정말 아름다운 색이에요. 청록색은 청색과 녹색이 섞인 색으로, 자연에서도 많이 볼 수 있어요. 예를 들어, 푸른 바다와 초록 나무들이 만나는 풍경이 청록색을 연상시킬 수 있어요. 청록색은 신비로운 느낌을 주기도 하고, 상쾌한 느낌을 주기도 해요. 여러분들이 좋아하는 색을 알려주셔서 감사해요. 저도 여러분들이 좋아하는 색을 알고 싶어요. 혹시 여러분들이 좋아하는 색은 무엇인가요?\n",
            "Human: 숭실대학교에 대해 잘 알아? 숭실대 근처 맛집 추천좀\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'네, 숭실대학교는 한국의 유명한 대학교 중 하나에요. 숭실대학교는 1946년에 설립되어 교육 및 연구 분야에서 우수한 성과를 내고 있어요. 숭실대학교 근처에는 맛집이 많이 있어요. 예를 들어, 숭실대학교 근처에는 다양한 음식점들이 있어 한식, 양식, 중식, 일식 등 다양한 요리를 즐길 수 있어요. 제가 추천해드릴 맛집은 \"숭실대 맛집\" 앱을 이용하시면 빠르고 편리하게 맛집을 찾아보실 수 있어요. 혹시 더 궁금한 점이 있으시면 언제든지 물어봐주세요.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv.predict(input='내가 좋아하는 색이 무슨 색이랬지?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "id": "pMhch-t1baq9",
        "outputId": "fb389253-7043-4a75-eee5-2a8253b80121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 숭실대학교에 대해 잘 알아? 숭실대 근처 맛집 추천좀\n",
            "AI: 네, 숭실대학교는 한국의 유명한 대학교 중 하나에요. 숭실대학교는 1946년에 설립되어 교육 및 연구 분야에서 우수한 성과를 내고 있어요. 숭실대학교 근처에는 맛집이 많이 있어요. 예를 들어, 숭실대학교 근처에는 다양한 음식점들이 있어 한식, 양식, 중식, 일식 등 다양한 요리를 즐길 수 있어요. 제가 추천해드릴 맛집은 \"숭실대 맛집\" 앱을 이용하시면 빠르고 편리하게 맛집을 찾아보실 수 있어요. 혹시 더 궁금한 점이 있으시면 언제든지 물어봐주세요.\n",
            "Human: 내가 좋아하는 색이 무슨 색이랬지?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'죄송해요, 당신이 좋아하는 색에 대해서는 정보가 없어요.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv.memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kGrNRgDb608",
        "outputId": "0ec859ba-3578-48f1-ac65-2c494f12aacf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferWindowMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='안녕. 내가 좋아하는 색은 청록색이야', additional_kwargs={}, response_metadata={}), AIMessage(content='안녕하세요! 청록색은 정말 아름다운 색이에요. 청록색은 청색과 녹색이 섞인 색으로, 자연에서도 많이 볼 수 있어요. 예를 들어, 푸른 바다와 초록 나무들이 만나는 풍경이 청록색을 연상시킬 수 있어요. 청록색은 신비로운 느낌을 주기도 하고, 상쾌한 느낌을 주기도 해요. 여러분들이 좋아하는 색을 알려주셔서 감사해요. 저도 여러분들이 좋아하는 색을 알고 싶어요. 혹시 여러분들이 좋아하는 색은 무엇인가요?', additional_kwargs={}, response_metadata={}), HumanMessage(content='숭실대학교에 대해 잘 알아? 숭실대 근처 맛집 추천좀', additional_kwargs={}, response_metadata={}), AIMessage(content='네, 숭실대학교는 한국의 유명한 대학교 중 하나에요. 숭실대학교는 1946년에 설립되어 교육 및 연구 분야에서 우수한 성과를 내고 있어요. 숭실대학교 근처에는 맛집이 많이 있어요. 예를 들어, 숭실대학교 근처에는 다양한 음식점들이 있어 한식, 양식, 중식, 일식 등 다양한 요리를 즐길 수 있어요. 제가 추천해드릴 맛집은 \"숭실대 맛집\" 앱을 이용하시면 빠르고 편리하게 맛집을 찾아보실 수 있어요. 혹시 더 궁금한 점이 있으시면 언제든지 물어봐주세요.', additional_kwargs={}, response_metadata={}), HumanMessage(content='내가 좋아하는 색이 무슨 색이랬지?', additional_kwargs={}, response_metadata={}), AIMessage(content='죄송해요, 당신이 좋아하는 색에 대해서는 정보가 없어요.', additional_kwargs={}, response_metadata={})]), k=1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 토큰 버퍼 메모리\n",
        "- 토큰을 저장하는 메모리(저장할 토큰의 수를 컨트롤하도록 되어 있음)\n",
        "- LLM은 토큰으로 비용을 계산하기 때문에, LLM 사용 비용 컨트롤에 영향을 미침"
      ],
      "metadata": {
        "id": "o4X5mApxcVna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#토큰을 컨트롤하는 라이브러리< RAG\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMPcIj5ecVXw",
        "outputId": "cd9675ce-4f51-4373-ae4f-e774b0b4a9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(temperature=0, model='gpt-4o-mini')\n",
        "#****max_token_limit -> 토큰의 길이제한\n",
        "memory = ConversationTokenBufferMemory(max_token_limit=50,\n",
        "                                       llm=llm)\n",
        "#바꾼 메모리 적용\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-tqf7WOcRYt",
        "outputId": "3af9b562-8bf1-4102-a12e-02be23046be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1773894231.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationTokenBufferMemory(max_token_limit=50,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ri0gdh1HVy8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#memory에 대화내역을 (인위적으로) 삽입\n",
        "memory.save_context({\"input\": \"AI is what?!\"}, #input -> Human\n",
        "                    {\"output\": \"Amazing!\"}) #output = AI\n",
        "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
        "                    {\"output\": \"Beautiful!\"})\n",
        "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
        "                    {\"output\": \"Charming!\"})"
      ],
      "metadata": {
        "id": "nEiIY8uFc-6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#메모리 기록 들춰보기\n",
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDcFRLcRdQDp",
        "outputId": "405353f5-e378-4e90-d2a1-2bb570ea21f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ConversationSummeryMemory"
      ],
      "metadata": {
        "id": "Q8zidtZfdyiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a long string\n",
        "schedule = \"\"\"오전 10시: 클라이언트와 온라인 미팅 (Zoom)\n",
        "→ 새 프로젝트 요구사항 논의\n",
        "→ 모델 구조 제안서 공유 예정이므로 미리 정리해 둘 것\n",
        "오후 1시 ~ 5시: 미팅 내용 기반으로 모델 프로토타입 개발\n",
        "→ 데이터 전처리 파이프라인 구성\n",
        "→ Transformer 기반 모델 베이스라인 학습 돌려보기\n",
        "→ 저녁 세미나 전에 체크포인트 저장 필수\n",
        "오후 7시: 대학원 세미나 발표 (오프라인)\n",
        "→ 발표 주제: \"Diffusion 모델의 최신 동향 및 응용 사례\"\n",
        "→ 발표 자료와 예제 코드 노트북 준비\n",
        "→ Q&A 대비해서 논문 내용 숙지해 둘 것\n",
        "※ 오전-오후-저녁까지 일정 가득함, 집중력과 체력 분배 중요\"\"\""
      ],
      "metadata": {
        "id": "WDp7i83vdut4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = ConversationSummaryMemory(llm=llm,\n",
        "                                  max_token_limit=25)\n",
        "\n",
        "conversation = ConversationSummaryMemory(llm=llm, memory=memory, verbose=True)"
      ],
      "metadata": {
        "id": "WZDSBUtQeC51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory.save_context({'input': 'What is on the schedule?'},\n",
        "                    {'output': f'{schedule}'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "RQZEveo2eWCV",
        "outputId": "68fd227e-c13f-4013-ce12-58fc60e448fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'schedule' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-676133654.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m memory.save_context({'input': 'What is on the schedule?'},\n\u001b[0;32m----> 2\u001b[0;31m                     {'output': f'{schedule}'})\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'schedule' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.load_memory_variables({})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIAdipMTevkP",
        "outputId": "aeee07a2-aedc-4f04-9f92-1b180d1ba69b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}