# -*- coding: utf-8 -*-
"""BM25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ao_zx064n3NYRGbwSAacBNKvq8h3mWn9
"""

#langchain과 관련된 베이스 라이브러리들.
!pip install langchain openai langchain-community

#tiktoken은 언어의 '토큰화'를 지원해줌
!pip install tiktoken

#bm25는 벡터의 유사도 검색을 잘 해내는 라이브러리임
!pip install rank_bm25

#트랜스포머를 기반으로 문장이나 문서를 임베딩할 수 있도록 돕는 라이브러리
!pip install sentence-transformers

#벡터 데이터를 저장함(벡터 DB)
!pip install chromadb

import tiktoken, openai

#'임베딩'을 담당하는 라이브러리.
#허깅페이스의 특정 모델을 기반으로 임베딩을 도와주는 것이 HuggingFace~~~Embeddings 임
from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain.embeddings import HuggingFaceBgeEmbeddings

#문서를 가져온 후, 문서를 일정한 길이로 잘라줌(Text->Splitter)
from langchain.text_splitter import RecursiveCharacterTextSplitter

#VectorDB. 데이터를 임베딩한 후 숫자로 변한 데이터를 저장하고,
#DB내에서 숫자 데이터의 검색을 더 빠르게 할 수 있도록 지원함
from langchain.vectorstores import Chroma
#from langchain_community.vectorstores import FAISS > cpu용 FAISS, gpu용 FAISS

#pdf 파일이나 문서 파일을 입력했을 때, 내부의 데이터를 가져옴
from langchain.document_loaders import TextLoader
from langchain.document_loaders import PyPDFLoader

#웹사이트 텍스트를 가져오는 역할
from langchain.document_loaders import WebBaseLoader

#토크나이저는 -> 토큰화를 시키는 것
#'cl100k_base' -> 토크나이저의 종류, gpt가 사용하는 토크나이저와 유사한 특징을 가짐
tokenizer = tiktoken.get_encoding('cl100k_base')

#인풋된 text가 정해진 토크나이저로 잘려 나가면, 그 길이를 알 수 있게 됨
def titoken_len(text):
  tokens = tokenizer.encode(text)
  return len(tokens)

#모델 다운로드
model_hug = HuggingFaceBgeEmbeddings(model_name='BAAI/bge-m3')

#자료 모으기!!
loaders=[
    WebBaseLoader("https://github.com/huggingface"),
    WebBaseLoader("https://wikidocs.net/229987"),
    WebBaseLoader("https://en.wikipedia.org/wiki/Hugging_Face"),
]

#데이터를 토큰화 하여 load 하기 위한 내용
docs = []

for load in loaders:
  #~~~Loader로 가져온 데이터에 대해 데이터를 불러오고 잘라줌 : load_and_split()
  docs.extend(load.load_and_split())

print(f'총 문서의 길이 : {len(docs)}')

docs[15].page_content

#토큰화(청크화)
#chunk_size = 문서(Document)를 얼마 단위로 자를까?
#chunk_overlap
#나는 코딩하는 것을 좋아합니다.
#나는 코딩하는 것을
#         하는 것을 좋아합니다.
text_splitter = RecursiveCharacterTextSplitter(chunk_size = 250,
                                               chunk_overlap = 20,
                                               length_function = titoken_len)

texts = text_splitter.split_documents(docs)
print(f'자른 내용 : {texts[0]}, 총 길이 {len(texts)}')

from langchain.retrievers import BM25Retriever

#BM25 TF-IDF(문서 빈도-역빈도)계열의 준수한 성능을 보이는 알고리즘
bm25_r = BM25Retriever.from_documents(texts)

#질문과 가장 닮은 내용을 담고 있는 top 3개를 뽑아라~
bm25_r.k = 3

result = bm25_r.invoke('What is the huggingface?')
result[1].page_content

import os
from google.colab import userdata

os.environ['OPENAI_API_KEY'] = userdata.get('ssu')

#리트리버 관련 모듈
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.chat_models import ChatOpenAI

#채팅할 대상인 llm 모델 객체 만듦
llm = ChatOpenAI()

#우리가 잘라 둔 text를 VectorDB에 넣어줌
#왜? 벡터(숫자)로 만들고 빠르게 '계산' 할 수 있게 하기 위해서 => '유사도' 검색
#Chroma dB에 texts를 model_hug 방법으로 임베딩해서, 넣어놓음
chroma_db = Chroma.from_documents(texts, model_hug)

#리트리버를 정의
multi_r = MultiQueryRetriever.from_llm(retriever=chroma_db.as_retriever(),
                                       llm=llm)

response = multi_r.invoke('What is the huggingface?')
print(response)

response[0].page_content

from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(llm=llm, #이야기를 나눌 llm
                                 #map-reduce(데이터 개수가 많을 때), refine(순차적으로 인지할게)
                                  chain_type='stuff', #내가 넣어줄 도큐먼트를 어떻게 인지할거냐. stuff(한꺼번에 볼게)
                                  retriever=multi_r)

response = qa('What is the huggingface?')
print(response['result'])

'''Hugging Face is a company that provides tools and platforms to support the development and deployment of machine learning models. They contribute significantly to AI research and practical application development by making it easier for anyone to work with machine learning models. Their philosophy includes encouraging community collaboration and sharing knowledge and resources through open-source technologies.

#langchain과 관련된 베이스 라이브러리들.
!pip install langchain openai langchain-community
#tiktoken은 언어의 '토큰화'를 지원해줌
!pip install tiktoken
#bm25는 벡터의 유사도 검색을 잘 해내는 라이브러리임
!pip install rank_bm25
#트랜스포머를 기반으로 문장이나 문서를 임베딩할 수 있도록 돕는 라이브러리
!pip install sentence-transformers
#벡터 데이터를 저장함(벡터 DB)
!pip install chromadb

!pip install PyPDF2 faiss-cpu

#VectorDB : faiss / faiss는 cpu, gpu 최적화 버전이 나누어져 있음
from langchain import FAISS
from PyPDF2 import PdfReader

#openai에게 질의응답
from langchain.chains.question_answering import load_qa_chain

#텍스트 스플리터
from langchain.text_splitter import CharacterTextSplitter

#텍스트 전처리 함수
def preprocess_text(text):
  #텍스트 청크화
  text_splitter = CharacterTextSplitter(
      separator="\n",      #가져온 텍스트가 무엇을 기준으로 나뉘어져 있는지?
      chunk_size=1000,     #올려둔 텍스트를 얼마만큼의 길이로 자를 것인가?
      chunk_overlap=200,   #청크를 얼마나 겹쳐서 자를 것인가?
      length_function=len
  )
  chunks = text_splitter.split_text(text)

  #임베딩
  embeddings = HuggingFaceBgeEmbeddings(model_name = 'BAAI/bge-m3')

  #FAISS VectorDB
  documents = FAISS.from_texts(chunks, embeddings)
  return documents

pdf_path = '/content/프롬프트엔지니어링.pdf'

texts=''
pdf_contents = PdfReader(pdf_path)

for page in pdf_contents.pages:
  texts += page.extract_text()

texts

from langchain.embeddings import HuggingFaceBgeEmbeddings

docs = preprocess_text(texts)

doc = docs.similarity_search("프롬프트 엔지니어링이 뭐야?")
print(doc)

import os
from google.colab import userdata

os.environ['OPEN_API_KEY'] = userdata.get('ssu')

from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(temperature=0) #temperature(0~1)  1에 가까워질수록 '창의적'
chain = load_qa_chain(llm, chain_type="stuff") #stuff - 주어진 문서를 한번에 고려하여 작업하겠다.

#벡터DB, 리트리버를 전달 -> input_documents
response = chain.run(input_documents=doc, question="프롬프트 엔지니어링이 뭐야?")
print(response)

#벡터 DB는 한번 만들고 계속 재활용
docs.save_local('/content/faiss')

# 저장할 때 썼던 동일 모델로 임베딩 초기화
embeddings = HuggingFaceBgeEmbeddings(model_name='BAAI/bge-m3')
# 로컬에 저장된 DB 불러오기
vectorstore = FAISS.load_local(
    "/content/faiss",
    embeddings,
    allow_dangerous_deserialization=True  # colab/jupyter에서는 보통 이 옵션 필요
)
# 검색 테스트
result = vectorstore.similarity_search("프롬프트 엔지니어링에서 쓸 수 있는 스킬들 뭐 없어?", k=2)
print(result)

